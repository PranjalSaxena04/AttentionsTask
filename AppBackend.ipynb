{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "48948f505df145d3baa623034fabf9ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15dabae3ac3c4075bd1f84d7638e1b3f",
              "IPY_MODEL_5f7d1caefbaa426ca6c1a289e57ceab1",
              "IPY_MODEL_476a14ba662445f9984b394132aca927"
            ],
            "layout": "IPY_MODEL_60c7d2a2d681437da0de6d8e98634c6b"
          }
        },
        "15dabae3ac3c4075bd1f84d7638e1b3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc85eaf3e8d34475a23b3633ad712a98",
            "placeholder": "​",
            "style": "IPY_MODEL_08246b82507847319160a86277e9731b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5f7d1caefbaa426ca6c1a289e57ceab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84b027602f9d451a9800f1d8d42d730f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a2e9ea0cd86426ca18c68bf3e370ce7",
            "value": 2
          }
        },
        "476a14ba662445f9984b394132aca927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75c70f50de774597b20546f235fe7d73",
            "placeholder": "​",
            "style": "IPY_MODEL_d3fcd881e85a4ee8b26fea58b39ed614",
            "value": " 2/2 [00:36&lt;00:00, 16.64s/it]"
          }
        },
        "60c7d2a2d681437da0de6d8e98634c6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc85eaf3e8d34475a23b3633ad712a98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08246b82507847319160a86277e9731b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84b027602f9d451a9800f1d8d42d730f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a2e9ea0cd86426ca18c68bf3e370ce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "75c70f50de774597b20546f235fe7d73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3fcd881e85a4ee8b26fea58b39ed614": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers transformers neo4j fitz feedparser requests torch pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1NjBHzzSvUN",
        "outputId": "5731926d-5b5e-4f73-ff79-265b98ba1fd4",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.10/dist-packages (5.26.0)\n",
            "Requirement already satisfied: fitz in /usr/local/lib/python3.10/dist-packages (0.0.1.dev2)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (6.0.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20240706)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2024.2)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.10/dist-packages (from fitz) (5.0.9)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.10/dist-packages (from fitz) (7.1.0)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.10/dist-packages (from fitz) (0.22.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from fitz) (5.3.2)\n",
            "Requirement already satisfied: nipype in /usr/local/lib/python3.10/dist-packages (from fitz) (1.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fitz) (2.2.2)\n",
            "Requirement already satisfied: pyxnat in /usr/local/lib/python3.10/dist-packages (from fitz) (1.6.2)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2->fitz) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (6.4.5)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (8.1.7)\n",
            "Requirement already satisfied: prov>=1.5.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (2.0.1)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (2.8.2)\n",
            "Requirement already satisfied: rdflib>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (6.3.2)\n",
            "Requirement already satisfied: simplejson>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.19.3)\n",
            "Requirement already satisfied: traits!=5.0,>=4.6 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (6.4.3)\n",
            "Requirement already satisfied: etelemetry>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (0.3.1)\n",
            "Requirement already satisfied: looseversion!=1.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (1.3.0)\n",
            "Requirement already satisfied: puremagic in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (1.28)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2024.2)\n",
            "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (5.3.0)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Requirement already satisfied: ci-info>=0.2 in /usr/local/lib/python3.10/dist-packages (from etelemetry>=0.2.0->nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.2->nipype->fitz) (1.16.0)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=5.0.0->nipype->fitz) (0.6.1)\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "token = \"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\"\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=token)\n",
        "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"  # Replace with your model name or path"
      ],
      "metadata": {
        "id": "H4xxVwf4gLRr",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# Initialize the model and tokenizer\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"  # Replace with your desired model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, device_map = \"cuda:0\")\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map = \"cuda:0\")  # Move model to GPU if available"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "48948f505df145d3baa623034fabf9ae",
            "15dabae3ac3c4075bd1f84d7638e1b3f",
            "5f7d1caefbaa426ca6c1a289e57ceab1",
            "476a14ba662445f9984b394132aca927",
            "60c7d2a2d681437da0de6d8e98634c6b",
            "fc85eaf3e8d34475a23b3633ad712a98",
            "08246b82507847319160a86277e9731b",
            "84b027602f9d451a9800f1d8d42d730f",
            "7a2e9ea0cd86426ca18c68bf3e370ce7",
            "75c70f50de774597b20546f235fe7d73",
            "d3fcd881e85a4ee8b26fea58b39ed614"
          ]
        },
        "id": "WuwZ9Lr4g2BM",
        "outputId": "06ec66f0-75be-4aae-d6b9-7cdcfbcfa659",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48948f505df145d3baa623034fabf9ae"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6dnky3ao_ak",
        "outputId": "c4f21c8e-4f1c-4e20-a0d8-fc18829ca977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.10/dist-packages (from arxiv) (6.0.11)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.32.3)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2024.8.30)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: arxiv\n",
            "Successfully installed arxiv-2.1.3\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-13T17:35:13.711690Z",
          "iopub.execute_input": "2024-11-13T17:35:13.712078Z",
          "iopub.status.idle": "2024-11-13T17:35:20.623343Z",
          "shell.execute_reply.started": "2024-11-13T17:35:13.712041Z",
          "shell.execute_reply": "2024-11-13T17:35:20.622525Z"
        },
        "id": "bh7kN3lVo_ak"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "date = []\n",
        "url = []"
      ],
      "metadata": {
        "id": "M3DN1rxsrD54"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "\n",
        "def search_arxiv(refined_keyword):\n",
        "    # Initialize the arXiv API client\n",
        "    client = arxiv.Client()\n",
        "\n",
        "    # Define the search query\n",
        "    search = arxiv.Search(\n",
        "        query=refined_keyword,\n",
        "        max_results=10,  # Adjust as needed\n",
        "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
        "    )\n",
        "\n",
        "    # Fetch and display the results\n",
        "    for result in client.results(search):\n",
        "        print(f\"Title: {result.title}\")\n",
        "        print(f\"Authors: {', '.join(author.name for author in result.authors)}\")\n",
        "        print(f\"Published: {result.published.date()}\")\n",
        "        print(f\"Summary: {result.summary[:500]}...\")  # Display the first 500 characters of the summary\n",
        "        print(f\"PDF: {result.pdf_url}\")\n",
        "        date.append(result.published.date())\n",
        "        url.append(result.pdf_url)\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    user_input = input(\"Enter a keyword to search for research papers: \")\n",
        "    search_arxiv(user_input)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-13T17:36:20.028016Z",
          "iopub.execute_input": "2024-11-13T17:36:20.028730Z",
          "iopub.status.idle": "2024-11-13T17:36:24.430115Z",
          "shell.execute_reply.started": "2024-11-13T17:36:20.028686Z",
          "shell.execute_reply": "2024-11-13T17:36:24.427861Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pkeyDj1o_ak",
        "outputId": "a080829e-93f0-47ca-9168-dd3ef49a86e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a keyword to search for research papers: quatum computing\n",
            "Title: Material Transforms from Disentangled NeRF Representations\n",
            "Authors: Ivan Lopes, Jean-François Lalonde, Raoul de Charette\n",
            "Published: 2024-11-12\n",
            "Summary: In this paper, we first propose a novel method for transferring material\n",
            "transformations across different scenes. Building on disentangled Neural\n",
            "Radiance Field (NeRF) representations, our approach learns to map Bidirectional\n",
            "Reflectance Distribution Functions (BRDF) from pairs of scenes observed in\n",
            "varying conditions, such as dry and wet. The learned transformations can then\n",
            "be applied to unseen scenes with similar materials, therefore effectively\n",
            "rendering the transformation learned with an ar...\n",
            "PDF: http://arxiv.org/pdf/2411.08037v1\n",
            "--------------------------------------------------------------------------------\n",
            "Title: Scaling Properties of Diffusion Models for Perceptual Tasks\n",
            "Authors: Rahul Ravishankar, Zeeshan Patel, Jathushan Rajasegaran, Jitendra Malik\n",
            "Published: 2024-11-12\n",
            "Summary: In this paper, we argue that iterative computation with diffusion models\n",
            "offers a powerful paradigm for not only generation but also visual perception\n",
            "tasks. We unify tasks such as depth estimation, optical flow, and segmentation\n",
            "under image-to-image translation, and show how diffusion models benefit from\n",
            "scaling training and test-time compute for these perception tasks. Through a\n",
            "careful analysis of these scaling behaviors, we present various techniques to\n",
            "efficiently train diffusion models for...\n",
            "PDF: http://arxiv.org/pdf/2411.08034v1\n",
            "--------------------------------------------------------------------------------\n",
            "Title: GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation\n",
            "Authors: Yushi Lan, Shangchen Zhou, Zhaoyang Lyu, Fangzhou Hong, Shuai Yang, Bo Dai, Xingang Pan, Chen Change Loy\n",
            "Published: 2024-11-12\n",
            "Summary: While 3D content generation has advanced significantly, existing methods\n",
            "still face challenges with input formats, latent space design, and output\n",
            "representations. This paper introduces a novel 3D generation framework that\n",
            "addresses these challenges, offering scalable, high-quality 3D generation with\n",
            "an interactive Point Cloud-structured Latent space. Our framework employs a\n",
            "Variational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal)\n",
            "renderings as input, using a unique latent space ...\n",
            "PDF: http://arxiv.org/pdf/2411.08033v1\n",
            "--------------------------------------------------------------------------------\n",
            "Title: Learning with Less: Knowledge Distillation from Large Language Models via Unlabeled Data\n",
            "Authors: Juanhui Li, Sreyashi Nag, Hui Liu, Xianfeng Tang, Sheikh Sarwar, Limeng Cui, Hansu Gu, Suhang Wang, Qi He, Jiliang Tang\n",
            "Published: 2024-11-12\n",
            "Summary: In real-world NLP applications, Large Language Models (LLMs) offer promising\n",
            "solutions due to their extensive training on vast datasets. However, the large\n",
            "size and high computation demands of LLMs limit their practicality in many\n",
            "applications, especially when further fine-tuning is required. To address these\n",
            "limitations, smaller models are typically preferred for deployment. However,\n",
            "their training is hindered by the scarcity of labeled data. In contrast,\n",
            "unlabeled data is often readily which c...\n",
            "PDF: http://arxiv.org/pdf/2411.08028v1\n",
            "--------------------------------------------------------------------------------\n",
            "Title: LLMPhy: Complex Physical Reasoning Using Large Language Models and World Models\n",
            "Authors: Anoop Cherian, Radu Corcodel, Siddarth Jain, Diego Romeres\n",
            "Published: 2024-11-12\n",
            "Summary: Physical reasoning is an important skill needed for robotic agents when\n",
            "operating in the real world. However, solving such reasoning problems often\n",
            "involves hypothesizing and reflecting over complex multi-body interactions\n",
            "under the effect of a multitude of physical forces and thus learning all such\n",
            "interactions poses a significant hurdle for state-of-the-art machine learning\n",
            "frameworks, including large language models (LLMs). To study this problem, we\n",
            "propose a new physical reasoning task and a...\n",
            "PDF: http://arxiv.org/pdf/2411.08027v1\n",
            "--------------------------------------------------------------------------------\n",
            "Title: Incentive Design with Spillovers\n",
            "Authors: Krishna Dasaratha, Benjamin Golub, Anant Shah\n",
            "Published: 2024-11-12\n",
            "Summary: A principal uses payments conditioned on stochastic outcomes of a team\n",
            "project to elicit costly effort from the team members. We develop a multi-agent\n",
            "generalization of a classic first-order approach to contract optimization by\n",
            "leveraging methods from network games. The main results characterize the\n",
            "optimal allocation of incentive pay across agents and outcomes. Incentive\n",
            "optimality requires equalizing, across agents, a product of (i) individual\n",
            "productivity (ii) organizational centrality and (i...\n",
            "PDF: http://arxiv.org/pdf/2411.08026v1\n",
            "--------------------------------------------------------------------------------\n",
            "Title: Leonardo vindicated: Pythagorean trees for minimal reconstruction of the natural branching structures\n",
            "Authors: Dymitr Ruta, Corrado Mio, Ernesto Damiani\n",
            "Published: 2024-11-12\n",
            "Summary: Trees continue to fascinate with their natural beauty and as engineering\n",
            "masterpieces optimal with respect to several independent criteria. Pythagorean\n",
            "tree is a well-known fractal design that realistically mimics the natural tree\n",
            "branching structures. We study various types of Pythagorean-like fractal trees\n",
            "with different shapes of the base, branching angles and relaxed scales in an\n",
            "attempt to identify and explain which variants are the closest match to the\n",
            "branching structures commonly observe...\n",
            "PDF: http://arxiv.org/pdf/2411.08024v1\n",
            "--------------------------------------------------------------------------------\n",
            "Title: Language Models as Causal Effect Generators\n",
            "Authors: Lucius E. J. Bynum, Kyunghyun Cho\n",
            "Published: 2024-11-12\n",
            "Summary: We present a framework for large language model (LLM) based data generation\n",
            "with controllable causal structure. In particular, we define a procedure for\n",
            "turning any language model and any directed acyclic graph (DAG) into a\n",
            "sequence-driven structural causal model (SD-SCM). Broadly speaking, an SD-SCM\n",
            "is a causal model with user-defined structure and LLM-defined structural\n",
            "equations. We characterize how an SD-SCM allows sampling from observational,\n",
            "interventional, and counterfactual distributions...\n",
            "PDF: http://arxiv.org/pdf/2411.08019v1\n",
            "--------------------------------------------------------------------------------\n",
            "Title: Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings\n",
            "Authors: Aditya Sanghi, Aliasghar Khani, Pradyumna Reddy, Arianna Rampini, Derek Cheung, Kamal Rahimi Malekshan, Kanika Madan, Hooman Shayani\n",
            "Published: 2024-11-12\n",
            "Summary: Large-scale 3D generative models require substantial computational resources\n",
            "yet often fall short in capturing fine details and complex geometries at high\n",
            "resolutions. We attribute this limitation to the inefficiency of current\n",
            "representations, which lack the compactness required to model the generative\n",
            "models effectively. To address this, we introduce a novel approach called\n",
            "Wavelet Latent Diffusion, or WaLa, that encodes 3D shapes into wavelet-based,\n",
            "compact latent encodings. Specifically, we ...\n",
            "PDF: http://arxiv.org/pdf/2411.08017v1\n",
            "--------------------------------------------------------------------------------\n",
            "Title: Artistic Neural Style Transfer Algorithms with Activation Smoothing\n",
            "Authors: Xiangtian Li, Han Cao, Zhaoyang Zhang, Jiacheng Hu, Yuhui Jin, Zihao Zhao\n",
            "Published: 2024-11-12\n",
            "Summary: The works of Gatys et al. demonstrated the capability of Convolutional Neural\n",
            "Networks (CNNs) in creating artistic style images. This process of transferring\n",
            "content images in different styles is called Neural Style Transfer (NST). In\n",
            "this paper, we re-implement image-based NST, fast NST, and arbitrary NST. We\n",
            "also explore to utilize ResNet with activation smoothing in NST. Extensive\n",
            "experimental results demonstrate that smoothing transformation can greatly\n",
            "improve the quality of stylization res...\n",
            "PDF: http://arxiv.org/pdf/2411.08014v1\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNKno9VVrbLe",
        "outputId": "c22abe63-fba1-4486-a49f-a1175b317b61"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['http://arxiv.org/pdf/2411.08037v1',\n",
              " 'http://arxiv.org/pdf/2411.08034v1',\n",
              " 'http://arxiv.org/pdf/2411.08033v1',\n",
              " 'http://arxiv.org/pdf/2411.08028v1',\n",
              " 'http://arxiv.org/pdf/2411.08027v1',\n",
              " 'http://arxiv.org/pdf/2411.08026v1',\n",
              " 'http://arxiv.org/pdf/2411.08024v1',\n",
              " 'http://arxiv.org/pdf/2411.08019v1',\n",
              " 'http://arxiv.org/pdf/2411.08017v1',\n",
              " 'http://arxiv.org/pdf/2411.08014v1']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pdfminer.high_level import extract_text\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import logging\n",
        "from neo4j import GraphDatabase\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import numpy as np\n",
        "\n",
        "# ==============================\n",
        "# 1. Configure Logging\n",
        "# ==============================\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename='pdf_processing.log',\n",
        "    filemode='a',\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    level=logging.INFO\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# 2. Define Helper Functions\n",
        "# ==============================\n",
        "\n",
        "def load_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Extract text from a PDF file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        str or None: Extracted text or None if extraction fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = extract_text(file_path)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to load {file_path} with pdfminer.six: {e}\")\n",
        "        return None\n",
        "\n",
        "def chunk_text(text, chunk_size=500):\n",
        "    \"\"\"\n",
        "    Split text into chunks of approximately `chunk_size` words.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to split.\n",
        "        chunk_size (int): Number of words per chunk.\n",
        "\n",
        "    Returns:\n",
        "        list of str: List of text chunks.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    chunks = [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Compute the cosine similarity between two vectors.\n",
        "\n",
        "    Args:\n",
        "        vec1 (array-like): First vector.\n",
        "        vec2 (array-like): Second vector.\n",
        "\n",
        "    Returns:\n",
        "        float: Cosine similarity between vec1 and vec2.\n",
        "    \"\"\"\n",
        "    vec1 = np.array(vec1)\n",
        "    vec2 = np.array(vec2)\n",
        "    norm1 = np.linalg.norm(vec1)\n",
        "    norm2 = np.linalg.norm(vec2)\n",
        "    if norm1 == 0.0 or norm2 == 0.0:\n",
        "        return 0.0\n",
        "    return np.dot(vec1, vec2) / (norm1 * norm2)\n",
        "\n",
        "def get_token_count(text, tokenizer):\n",
        "    \"\"\"\n",
        "    Estimate the number of tokens in a given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to tokenize.\n",
        "        tokenizer: The tokenizer instance.\n",
        "\n",
        "    Returns:\n",
        "        int: Number of tokens.\n",
        "    \"\"\"\n",
        "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
        "\n",
        "# ==============================\n",
        "# 3. Initialize Embedding Model\n",
        "# ==============================\n",
        "\n",
        "logging.info(\"Loading embedding model...\")\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')  # Ensure consistency with embedding used earlier\n",
        "logging.info(\"Embedding model loaded successfully.\")\n",
        "\n",
        "# ==============================\n",
        "# 4. Initialize Neo4j Driver\n",
        "# ==============================\n",
        "\n",
        "# Replace with your Neo4j credentials\n",
        "uri = \"neo4j+s://63a38802.databases.neo4j.io\"  # Update if using Neo4j cloud\n",
        "username = \"neo4j\"\n",
        "password = \"LgpcsNosFzKkuEXXJWH7DeZ3cmw_Kyaj_F5LzcDNbXI\"\n",
        "\n",
        "try:\n",
        "    driver = GraphDatabase.driver(uri, auth=(username, password))\n",
        "    logging.info(\"Connected to Neo4j successfully.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Failed to connect to Neo4j: {e}\")\n",
        "    raise e\n",
        "\n",
        "# ==============================\n",
        "# 5. Load and Chunk PDFs\n",
        "# ==============================\n",
        "\n",
        "all_chunks = []\n",
        "doc_directory = \"/content/attentions\"\n",
        "\n",
        "if not os.path.exists(doc_directory):\n",
        "    logging.error(f\"Directory '{doc_directory}' does not exist.\")\n",
        "    raise FileNotFoundError(f\"Directory '{doc_directory}' does not exist.\")\n",
        "\n",
        "for file_name in os.listdir(doc_directory):\n",
        "    if file_name.lower().endswith(\".pdf\"):\n",
        "        file_path = os.path.join(doc_directory, file_name)\n",
        "        logging.info(f\"Processing file: {file_path}\")\n",
        "        text = load_pdf(file_path)\n",
        "        if text:\n",
        "            chunks = chunk_text(text, chunk_size=500)\n",
        "            all_chunks.extend([(file_name, chunk) for chunk in chunks])  # store as (file_name, chunk)\n",
        "        else:\n",
        "            logging.warning(f\"Skipping file due to load failure: {file_path}\")\n",
        "\n",
        "# Optionally, you can print or process all_chunks further\n",
        "print(f\"Total chunks created: {len(all_chunks)}\")\n",
        "\n",
        "# ==============================\n",
        "# 6. Create Embeddings and Store in Neo4j\n",
        "# ==============================\n",
        "\n",
        "# Load an embedding model\n",
        "logging.info(\"Loading language model and tokenizer...\")\n",
        "\n",
        "# Initialize the model and tokenizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create embeddings for each chunk\n",
        "chunk_data = []\n",
        "\n",
        "logging.info(\"Generating embeddings for each chunk...\")\n",
        "for file_name, chunk in all_chunks:\n",
        "    embedding = embedder.encode(chunk).tolist()  # Convert numpy array to list for JSON serialization\n",
        "    chunk_data.append((file_name, chunk, embedding))\n",
        "logging.info(\"Embeddings generated successfully.\")\n",
        "\n",
        "# Function to create graph nodes and relationships\n",
        "def create_graph(tx, document_name, chunk_text, embedding):\n",
        "    \"\"\"\n",
        "    Create Document and Chunk nodes and establish HAS_CHUNK relationship.\n",
        "\n",
        "    Args:\n",
        "        tx: Neo4j transaction.\n",
        "        document_name (str): Name of the document.\n",
        "        chunk_text (str): Text content of the chunk.\n",
        "        embedding (list): Embedding vector of the chunk.\n",
        "    \"\"\"\n",
        "    # Create or match the Document node\n",
        "    tx.run(\"\"\"\n",
        "        MERGE (d:Document {name: $document_name})\n",
        "    \"\"\", document_name=document_name)\n",
        "\n",
        "    # Create the Chunk node with properties\n",
        "    tx.run(\"\"\"\n",
        "        CREATE (c:Chunk {text: $chunk_text, embedding: $embedding})\n",
        "    \"\"\", chunk_text=chunk_text, embedding=embedding)\n",
        "\n",
        "    # Create relationship between Document and Chunk\n",
        "    tx.run(\"\"\"\n",
        "        MATCH (d:Document {name: $document_name}), (c:Chunk {text: $chunk_text})\n",
        "        MERGE (d)-[:HAS_CHUNK]->(c)\n",
        "    \"\"\", document_name=document_name, chunk_text=chunk_text)\n",
        "\n",
        "# Function to batch insert chunks into Neo4j\n",
        "def insert_batch(tx, batch):\n",
        "    for document_name, chunk_text, embedding in batch:\n",
        "        create_graph(tx, document_name, chunk_text, embedding)\n",
        "\n",
        "def batch_insert_chunks(chunk_data, batch_size=100):\n",
        "    \"\"\"\n",
        "    Insert chunk data into Neo4j in batches.\n",
        "\n",
        "    Args:\n",
        "        chunk_data (list): List of tuples containing (document_name, chunk_text, embedding).\n",
        "        batch_size (int): Number of chunks per batch.\n",
        "    \"\"\"\n",
        "    with driver.session() as session:\n",
        "        for i in range(0, len(chunk_data), batch_size):\n",
        "            batch = chunk_data[i:i + batch_size]\n",
        "            logging.info(f\"Inserting batch {i // batch_size + 1}\")\n",
        "            session.write_transaction(insert_batch, batch)\n",
        "\n",
        "# Insert all chunk data into Neo4j\n",
        "batch_insert_chunks(chunk_data)\n",
        "\n",
        "# ==============================\n",
        "# 7. Define Response Generation Functions\n",
        "# ==============================\n",
        "\n",
        "def get_relevant_chunks(query, top_k=5):\n",
        "    \"\"\"\n",
        "    Retrieve the top_k most relevant chunks from Neo4j based on the query.\n",
        "\n",
        "    Args:\n",
        "        query (str): User's query.\n",
        "        top_k (int): Number of top similar chunks to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        list of dict: List containing dictionaries with 'document', 'text', and 'similarity'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Generate embedding for the query\n",
        "        query_embedding = embedder.encode(query)\n",
        "\n",
        "        with driver.session() as session:\n",
        "            # Retrieve all chunks and their embeddings\n",
        "            result = session.run(\"\"\"\n",
        "                MATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk)\n",
        "                RETURN d.name AS document, c.text AS text, c.embedding AS embedding\n",
        "            \"\"\")\n",
        "\n",
        "            chunks = []\n",
        "            for record in result:\n",
        "                chunk_embedding = record[\"embedding\"]\n",
        "                # Compute cosine similarity using custom function\n",
        "                similarity = cosine_similarity(query_embedding, chunk_embedding)\n",
        "                chunks.append({\n",
        "                    \"document\": record[\"document\"],\n",
        "                    \"text\": record[\"text\"],\n",
        "                    \"similarity\": similarity\n",
        "                })\n",
        "\n",
        "        # Sort chunks by similarity and return top_k\n",
        "        sorted_chunks = sorted(chunks, key=lambda x: x[\"similarity\"], reverse=True)\n",
        "        return sorted_chunks[:top_k]\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error retrieving chunks: {e}\")\n",
        "        return []\n",
        "\n",
        "def generate_response(query, top_k=5, max_new_tokens=512, max_total_tokens=4096):\n",
        "    \"\"\"\n",
        "    Generate a response to the user's query based on relevant chunks.\n",
        "\n",
        "    Args:\n",
        "        query (str): User's query.\n",
        "        top_k (int): Number of top similar chunks to use as context.\n",
        "        max_new_tokens (int): Maximum number of tokens to generate.\n",
        "        max_total_tokens (int): Maximum total tokens (input + generated).\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Retrieve relevant chunks\n",
        "        relevant_chunks = get_relevant_chunks(query, top_k=top_k)\n",
        "\n",
        "        # Construct the context incrementally to fit within max_total_tokens\n",
        "        context = \"\"\n",
        "        for chunk in relevant_chunks:\n",
        "            potential_context = context + f\"\\n\\nDocument: {chunk['document']}\\nContent: {chunk['text']}\"\n",
        "            prompt_preview = f\"Context:{potential_context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "            total_tokens = get_token_count(prompt_preview, tokenizer) + max_new_tokens\n",
        "            if total_tokens <= max_total_tokens:\n",
        "                context = potential_context\n",
        "            else:\n",
        "                break  # Stop adding chunks if adding more would exceed the token limit\n",
        "\n",
        "        # Create the prompt\n",
        "        prompt = f\"Context:{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "        # Tokenize and generate\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,  # Use max_new_tokens instead of max_length\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Decode the response\n",
        "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract the answer part\n",
        "        answer = answer.split(\"Answer:\")[-1].strip()\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating response: {e}\")\n",
        "        return \"I'm sorry, I encountered an error while generating the response.\""
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfqtbh0to_an",
        "outputId": "2755e2ac-14ec-4c10-b8af-27c86244df56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks created: 0\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 8. Example Usage\n",
        "# ==============================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        user_query = input(\"Enter your query: \")\n",
        "        response = generate_response(user_query)\n",
        "        print(\"\\nGenerated Response:\\n\", response)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Unexpected error: {e}\")\n",
        "        print(\"An unexpected error occurred. Please try again later.\")\n",
        "\n",
        "# ==============================\n",
        "# 9. Close Neo4j Driver Properly\n",
        "# ==============================\n",
        "\n",
        "# It's recommended to close the driver when all operations are done.\n",
        "# If you're using this script as a standalone application, you can close the driver here.\n",
        "# However, if you're using it in a Jupyter notebook or an interactive environment,\n",
        "# you might want to manage the driver's lifecycle differently.\n",
        "\n",
        "try:\n",
        "    driver.close()\n",
        "    logging.info(\"Neo4j driver closed successfully.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error closing Neo4j driver: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru1qR8eWo_ao",
        "outputId": "84cd3345-59fc-4285-c433-374c75791520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: What is quantum computing\n",
            "\n",
            "Generated Response:\n",
            " Quantum computing is a new paradigm for computation that uses the principles of quantum mechanics, such as superposition and entanglement, to perform calculations and operations on data.\n",
            "\n",
            "The text provided does not mention quantum computing at all. It appears to be a collection of papers and articles related to computer vision and object geo-localization. If you'd like to ask a question related to the provided text, I'd be happy to help.\n"
          ]
        }
      ],
      "execution_count": 8
    }
  ]
}