{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4d816ba80ed74dec981ab6318578bd56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_886a991109fb4c4e9eef97c5b1c0d802",
              "IPY_MODEL_b394df77076e4295b21fceb995bb0efa",
              "IPY_MODEL_ff9fdf03581d4f9787ef03401c616da7"
            ],
            "layout": "IPY_MODEL_011a57c297db4768b45bca0430ce0654"
          }
        },
        "886a991109fb4c4e9eef97c5b1c0d802": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eed0eaf92da548e5853f9e4c64a781bd",
            "placeholder": "​",
            "style": "IPY_MODEL_949e6a4360aa45cca68cba8d7c921155",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "b394df77076e4295b21fceb995bb0efa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4a9e09361e5499a9b8e36dddec936db",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eaa482f8a20544479efee453e07b0151",
            "value": 0
          }
        },
        "ff9fdf03581d4f9787ef03401c616da7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b81abd6eff64c37a2ec09befcfa185d",
            "placeholder": "​",
            "style": "IPY_MODEL_84eaf1ea822f469db9fcf7e34f3c525a",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "011a57c297db4768b45bca0430ce0654": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eed0eaf92da548e5853f9e4c64a781bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "949e6a4360aa45cca68cba8d7c921155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4a9e09361e5499a9b8e36dddec936db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaa482f8a20544479efee453e07b0151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b81abd6eff64c37a2ec09befcfa185d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84eaf1ea822f469db9fcf7e34f3c525a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers transformers neo4j fitz feedparser requests torch pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1NjBHzzSvUN",
        "outputId": "cf818a8a-b2de-4420-9538-c8e8c671d64c",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.10/dist-packages (5.26.0)\n",
            "Requirement already satisfied: fitz in /usr/local/lib/python3.10/dist-packages (0.0.1.dev2)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (6.0.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20240706)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2024.2)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.10/dist-packages (from fitz) (5.0.9)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.10/dist-packages (from fitz) (7.1.0)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.10/dist-packages (from fitz) (0.22.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from fitz) (5.3.2)\n",
            "Requirement already satisfied: nipype in /usr/local/lib/python3.10/dist-packages (from fitz) (1.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fitz) (2.2.2)\n",
            "Requirement already satisfied: pyxnat in /usr/local/lib/python3.10/dist-packages (from fitz) (1.6.2)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2->fitz) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (6.4.5)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (8.1.7)\n",
            "Requirement already satisfied: prov>=1.5.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (2.0.1)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (2.8.2)\n",
            "Requirement already satisfied: rdflib>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (6.3.2)\n",
            "Requirement already satisfied: simplejson>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.19.3)\n",
            "Requirement already satisfied: traits!=5.0,>=4.6 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (6.4.3)\n",
            "Requirement already satisfied: etelemetry>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (0.3.1)\n",
            "Requirement already satisfied: looseversion!=1.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (1.3.0)\n",
            "Requirement already satisfied: puremagic in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (1.28)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2024.2)\n",
            "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (5.3.0)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Requirement already satisfied: ci-info>=0.2 in /usr/local/lib/python3.10/dist-packages (from etelemetry>=0.2.0->nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.2->nipype->fitz) (1.16.0)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=5.0.0->nipype->fitz) (0.6.1)\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "token = \"YOUR_API_KEY\"\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=token)\n",
        "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"  # Replace with your model name or path"
      ],
      "metadata": {
        "id": "H4xxVwf4gLRr",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# Initialize the model and tokenizer\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"  # Replace with your desired model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, device_map = \"cuda:0\")\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map = \"cuda:0\")  # Move model to GPU if available"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "4d816ba80ed74dec981ab6318578bd56",
            "886a991109fb4c4e9eef97c5b1c0d802",
            "b394df77076e4295b21fceb995bb0efa",
            "ff9fdf03581d4f9787ef03401c616da7",
            "011a57c297db4768b45bca0430ce0654",
            "eed0eaf92da548e5853f9e4c64a781bd",
            "949e6a4360aa45cca68cba8d7c921155",
            "c4a9e09361e5499a9b8e36dddec936db",
            "eaa482f8a20544479efee453e07b0151",
            "6b81abd6eff64c37a2ec09befcfa185d",
            "84eaf1ea822f469db9fcf7e34f3c525a"
          ]
        },
        "id": "WuwZ9Lr4g2BM",
        "outputId": "59fac9a8-4e25-40f2-9ccf-c91ce35509ec",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d816ba80ed74dec981ab6318578bd56"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv"
      ],
      "metadata": {
        "trusted": true,
        "id": "_6dnky3ao_ak"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-13T17:35:13.711690Z",
          "iopub.execute_input": "2024-11-13T17:35:13.712078Z",
          "iopub.status.idle": "2024-11-13T17:35:20.623343Z",
          "shell.execute_reply.started": "2024-11-13T17:35:13.712041Z",
          "shell.execute_reply": "2024-11-13T17:35:20.622525Z"
        },
        "id": "bh7kN3lVo_ak"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "date = []\n",
        "url = []"
      ],
      "metadata": {
        "id": "M3DN1rxsrD54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "\n",
        "def search_arxiv(refined_keyword):\n",
        "    # Initialize the arXiv API client\n",
        "    client = arxiv.Client()\n",
        "\n",
        "    # Define the search query\n",
        "    search = arxiv.Search(\n",
        "        query=refined_keyword,\n",
        "        max_results=10,  # Adjust as needed\n",
        "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
        "    )\n",
        "\n",
        "    # Fetch and display the results\n",
        "    for result in client.results(search):\n",
        "        print(f\"Title: {result.title}\")\n",
        "        print(f\"Authors: {', '.join(author.name for author in result.authors)}\")\n",
        "        print(f\"Published: {result.published.date()}\")\n",
        "        print(f\"Summary: {result.summary[:500]}...\")  # Display the first 500 characters of the summary\n",
        "        print(f\"PDF: {result.pdf_url}\")\n",
        "        date.append(result.published.date())\n",
        "        url.append(result.pdf_url)\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    user_input = input(\"Enter a keyword to search for research papers: \")\n",
        "    search_arxiv(user_input)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-13T17:36:20.028016Z",
          "iopub.execute_input": "2024-11-13T17:36:20.028730Z",
          "iopub.status.idle": "2024-11-13T17:36:24.430115Z",
          "shell.execute_reply.started": "2024-11-13T17:36:20.028686Z",
          "shell.execute_reply": "2024-11-13T17:36:24.427861Z"
        },
        "id": "6pkeyDj1o_ak"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "url"
      ],
      "metadata": {
        "id": "xNKno9VVrbLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pdfminer.high_level import extract_text\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import logging\n",
        "from neo4j import GraphDatabase\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import numpy as np\n",
        "\n",
        "# ==============================\n",
        "# 1. Configure Logging\n",
        "# ==============================\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename='pdf_processing.log',\n",
        "    filemode='a',\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    level=logging.INFO\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# 2. Define Helper Functions\n",
        "# ==============================\n",
        "\n",
        "def load_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Extract text from a PDF file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        str or None: Extracted text or None if extraction fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = extract_text(file_path)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to load {file_path} with pdfminer.six: {e}\")\n",
        "        return None\n",
        "\n",
        "def chunk_text(text, chunk_size=500):\n",
        "    \"\"\"\n",
        "    Split text into chunks of approximately `chunk_size` words.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to split.\n",
        "        chunk_size (int): Number of words per chunk.\n",
        "\n",
        "    Returns:\n",
        "        list of str: List of text chunks.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    chunks = [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Compute the cosine similarity between two vectors.\n",
        "\n",
        "    Args:\n",
        "        vec1 (array-like): First vector.\n",
        "        vec2 (array-like): Second vector.\n",
        "\n",
        "    Returns:\n",
        "        float: Cosine similarity between vec1 and vec2.\n",
        "    \"\"\"\n",
        "    vec1 = np.array(vec1)\n",
        "    vec2 = np.array(vec2)\n",
        "    norm1 = np.linalg.norm(vec1)\n",
        "    norm2 = np.linalg.norm(vec2)\n",
        "    if norm1 == 0.0 or norm2 == 0.0:\n",
        "        return 0.0\n",
        "    return np.dot(vec1, vec2) / (norm1 * norm2)\n",
        "\n",
        "def get_token_count(text, tokenizer):\n",
        "    \"\"\"\n",
        "    Estimate the number of tokens in a given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to tokenize.\n",
        "        tokenizer: The tokenizer instance.\n",
        "\n",
        "    Returns:\n",
        "        int: Number of tokens.\n",
        "    \"\"\"\n",
        "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
        "\n",
        "# ==============================\n",
        "# 3. Initialize Embedding Model\n",
        "# ==============================\n",
        "\n",
        "logging.info(\"Loading embedding model...\")\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')  # Ensure consistency with embedding used earlier\n",
        "logging.info(\"Embedding model loaded successfully.\")\n",
        "\n",
        "# ==============================\n",
        "# 4. Initialize Neo4j Driver\n",
        "# ==============================\n",
        "\n",
        "# Replace with your Neo4j credentials\n",
        "uri = \"neo4j+s://63a38802.databases.neo4j.io\"  # Update if using Neo4j cloud\n",
        "username = \"neo4j\"\n",
        "password = \"LgpcsNosFzKkuEXXJWH7DeZ3cmw_Kyaj_F5LzcDNbXI\"\n",
        "\n",
        "try:\n",
        "    driver = GraphDatabase.driver(uri, auth=(username, password))\n",
        "    logging.info(\"Connected to Neo4j successfully.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Failed to connect to Neo4j: {e}\")\n",
        "    raise e\n",
        "\n",
        "# ==============================\n",
        "# 5. Load and Chunk PDFs\n",
        "# ==============================\n",
        "\n",
        "all_chunks = []\n",
        "doc_directory = \"/content/attentions\"\n",
        "\n",
        "if not os.path.exists(doc_directory):\n",
        "    logging.error(f\"Directory '{doc_directory}' does not exist.\")\n",
        "    raise FileNotFoundError(f\"Directory '{doc_directory}' does not exist.\")\n",
        "\n",
        "for file_name in os.listdir(doc_directory):\n",
        "    if file_name.lower().endswith(\".pdf\"):\n",
        "        file_path = os.path.join(doc_directory, file_name)\n",
        "        logging.info(f\"Processing file: {file_path}\")\n",
        "        text = load_pdf(file_path)\n",
        "        if text:\n",
        "            chunks = chunk_text(text, chunk_size=500)\n",
        "            all_chunks.extend([(file_name, chunk) for chunk in chunks])  # store as (file_name, chunk)\n",
        "        else:\n",
        "            logging.warning(f\"Skipping file due to load failure: {file_path}\")\n",
        "\n",
        "# Optionally, you can print or process all_chunks further\n",
        "print(f\"Total chunks created: {len(all_chunks)}\")\n",
        "\n",
        "# ==============================\n",
        "# 6. Create Embeddings and Store in Neo4j\n",
        "# ==============================\n",
        "\n",
        "# Load an embedding model\n",
        "logging.info(\"Loading language model and tokenizer...\")\n",
        "\n",
        "# Initialize the model and tokenizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create embeddings for each chunk\n",
        "chunk_data = []\n",
        "\n",
        "logging.info(\"Generating embeddings for each chunk...\")\n",
        "for file_name, chunk in all_chunks:\n",
        "    embedding = embedder.encode(chunk).tolist()  # Convert numpy array to list for JSON serialization\n",
        "    chunk_data.append((file_name, chunk, embedding))\n",
        "logging.info(\"Embeddings generated successfully.\")\n",
        "\n",
        "# Function to create graph nodes and relationships\n",
        "def create_graph(tx, document_name, chunk_text, embedding):\n",
        "    \"\"\"\n",
        "    Create Document and Chunk nodes and establish HAS_CHUNK relationship.\n",
        "\n",
        "    Args:\n",
        "        tx: Neo4j transaction.\n",
        "        document_name (str): Name of the document.\n",
        "        chunk_text (str): Text content of the chunk.\n",
        "        embedding (list): Embedding vector of the chunk.\n",
        "    \"\"\"\n",
        "    # Create or match the Document node\n",
        "    tx.run(\"\"\"\n",
        "        MERGE (d:Document {name: $document_name})\n",
        "    \"\"\", document_name=document_name)\n",
        "\n",
        "    # Create the Chunk node with properties\n",
        "    tx.run(\"\"\"\n",
        "        CREATE (c:Chunk {text: $chunk_text, embedding: $embedding})\n",
        "    \"\"\", chunk_text=chunk_text, embedding=embedding)\n",
        "\n",
        "    # Create relationship between Document and Chunk\n",
        "    tx.run(\"\"\"\n",
        "        MATCH (d:Document {name: $document_name}), (c:Chunk {text: $chunk_text})\n",
        "        MERGE (d)-[:HAS_CHUNK]->(c)\n",
        "    \"\"\", document_name=document_name, chunk_text=chunk_text)\n",
        "\n",
        "# Function to batch insert chunks into Neo4j\n",
        "def insert_batch(tx, batch):\n",
        "    for document_name, chunk_text, embedding in batch:\n",
        "        create_graph(tx, document_name, chunk_text, embedding)\n",
        "\n",
        "def batch_insert_chunks(chunk_data, batch_size=100):\n",
        "    \"\"\"\n",
        "    Insert chunk data into Neo4j in batches.\n",
        "\n",
        "    Args:\n",
        "        chunk_data (list): List of tuples containing (document_name, chunk_text, embedding).\n",
        "        batch_size (int): Number of chunks per batch.\n",
        "    \"\"\"\n",
        "    with driver.session() as session:\n",
        "        for i in range(0, len(chunk_data), batch_size):\n",
        "            batch = chunk_data[i:i + batch_size]\n",
        "            logging.info(f\"Inserting batch {i // batch_size + 1}\")\n",
        "            session.write_transaction(insert_batch, batch)\n",
        "\n",
        "# Insert all chunk data into Neo4j\n",
        "batch_insert_chunks(chunk_data)\n",
        "\n",
        "# ==============================\n",
        "# 7. Define Response Generation Functions\n",
        "# ==============================\n",
        "\n",
        "def get_relevant_chunks(query, top_k=5):\n",
        "    \"\"\"\n",
        "    Retrieve the top_k most relevant chunks from Neo4j based on the query.\n",
        "\n",
        "    Args:\n",
        "        query (str): User's query.\n",
        "        top_k (int): Number of top similar chunks to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        list of dict: List containing dictionaries with 'document', 'text', and 'similarity'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Generate embedding for the query\n",
        "        query_embedding = embedder.encode(query)\n",
        "\n",
        "        with driver.session() as session:\n",
        "            # Retrieve all chunks and their embeddings\n",
        "            result = session.run(\"\"\"\n",
        "                MATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk)\n",
        "                RETURN d.name AS document, c.text AS text, c.embedding AS embedding\n",
        "            \"\"\")\n",
        "\n",
        "            chunks = []\n",
        "            for record in result:\n",
        "                chunk_embedding = record[\"embedding\"]\n",
        "                # Compute cosine similarity using custom function\n",
        "                similarity = cosine_similarity(query_embedding, chunk_embedding)\n",
        "                chunks.append({\n",
        "                    \"document\": record[\"document\"],\n",
        "                    \"text\": record[\"text\"],\n",
        "                    \"similarity\": similarity\n",
        "                })\n",
        "\n",
        "        # Sort chunks by similarity and return top_k\n",
        "        sorted_chunks = sorted(chunks, key=lambda x: x[\"similarity\"], reverse=True)\n",
        "        return sorted_chunks[:top_k]\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error retrieving chunks: {e}\")\n",
        "        return []\n",
        "\n",
        "def generate_response(query, top_k=5, max_new_tokens=512, max_total_tokens=4096):\n",
        "    \"\"\"\n",
        "    Generate a response to the user's query based on relevant chunks.\n",
        "\n",
        "    Args:\n",
        "        query (str): User's query.\n",
        "        top_k (int): Number of top similar chunks to use as context.\n",
        "        max_new_tokens (int): Maximum number of tokens to generate.\n",
        "        max_total_tokens (int): Maximum total tokens (input + generated).\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Retrieve relevant chunks\n",
        "        relevant_chunks = get_relevant_chunks(query, top_k=top_k)\n",
        "\n",
        "        # Construct the context incrementally to fit within max_total_tokens\n",
        "        context = \"\"\n",
        "        for chunk in relevant_chunks:\n",
        "            potential_context = context + f\"\\n\\nDocument: {chunk['document']}\\nContent: {chunk['text']}\"\n",
        "            prompt_preview = f\"Context:{potential_context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "            total_tokens = get_token_count(prompt_preview, tokenizer) + max_new_tokens\n",
        "            if total_tokens <= max_total_tokens:\n",
        "                context = potential_context\n",
        "            else:\n",
        "                break  # Stop adding chunks if adding more would exceed the token limit\n",
        "\n",
        "        # Create the prompt\n",
        "        prompt = f\"Context:{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "        # Tokenize and generate\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,  # Use max_new_tokens instead of max_length\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Decode the response\n",
        "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract the answer part\n",
        "        answer = answer.split(\"Answer:\")[-1].strip()\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating response: {e}\")\n",
        "        return \"I'm sorry, I encountered an error while generating the response.\""
      ],
      "metadata": {
        "trusted": true,
        "id": "xfqtbh0to_an"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 8. Example Usage\n",
        "# ==============================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        user_query = input(\"Enter your query: \")\n",
        "        response = generate_response(user_query)\n",
        "        print(\"\\nGenerated Response:\\n\", response)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Unexpected error: {e}\")\n",
        "        print(\"An unexpected error occurred. Please try again later.\")\n",
        "\n",
        "# ==============================\n",
        "# 9. Close Neo4j Driver Properly\n",
        "# ==============================\n",
        "\n",
        "# It's recommended to close the driver when all operations are done.\n",
        "# If you're using this script as a standalone application, you can close the driver here.\n",
        "# However, if you're using it in a Jupyter notebook or an interactive environment,\n",
        "# you might want to manage the driver's lifecycle differently.\n",
        "\n",
        "try:\n",
        "    driver.close()\n",
        "    logging.info(\"Neo4j driver closed successfully.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error closing Neo4j driver: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "ru1qR8eWo_ao"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}